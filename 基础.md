# 强化学习



[toc]

## 说明

Note为一些问题和主要知识点的汇总

思维导图

![img](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/dc4d2f95-d834-4439-868d-0c64e9aacbab/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20201019%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20201019T072607Z&X-Amz-Expires=86400&X-Amz-Signature=72386fc33e46af8a10bf3177f5f072fcaa60e947ba4af66b25d959a63bd0c937&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)



## 基础概念

强化学习概念图

![概念图](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/abb2794c-a658-4a29-bcc1-436db42f4606/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20201019%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20201019T065540Z&X-Amz-Expires=86400&X-Amz-Signature=b559d965eb9eb29fcbeb03d29fd8a1600bbee69362cb579dd5c242d0d020a2c1&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.1.png)

![img](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/bb98e2ec-f6ff-4d16-9a68-46eb2c168308/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20201019%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20201019T065618Z&X-Amz-Expires=86400&X-Amz-Signature=b393c5887fc586199b7291d4d73e6bca57db7344aa5b0e37e598e5fc3ac9ec72&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)

在每一步t，智能体：获得观察O_t,获得奖励R_t,执行行动A_t，环境：获得行动A_t,给出观察O_{t+1},给出奖励R_{t+1}

一个 智能体(agent) 怎么在一个复杂不确定的环境(environment)里面去极大化它能获得的奖励.

直观简单的快速了解可以看[莫烦python](https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-RL/)的视频，了解一个强化学习的概念。（不涉及数学公式推导，作为辅助理解。

### 主要元素

1. environment
2. Agent

### 主要流程

Agent 在环境里面获取到状态，agent 会利用这个状态输出一个 action，一个决策。然后这个决策会放到环境之中去，环境会通过这个 agent 采取的决策，输出下一个状态以及当前的这个决策得到的奖励。

### 目标

最大化Reward

### Note

- 由于不断地交互，数据具有强相关性。为序列数据。

- [ ] 学习到什么？智能体的输出是什么？

- [ ] 奖励是什么？需要怎么获取？



## Sequential Decision Making

Sequential Decision Making（序列决策过程）

### 奖励的描述

- [x] **奖励是环境给的一个反馈信号**，这个信号指定了这个 agent 在某一步采取了某个策略是否得到奖励。**奖励可正可负**。分为正向奖励和负向奖励。

- [ ] 奖励分为即时奖励和长远奖励。需要在这两者之间取得平衡。如何在这两者之间取得平衡，在后续马尔可夫过程中详细展开。强化学习里面一个重要的课题就是近期奖励和远期奖励的一个 trade-off。

### 序列信息是什么？

在跟环境的交互过程中，agent 会获得很多观测。在每一个观测会采取一个动作，它也会得到一个奖励。**所以历史是观测、行为、奖励的序列：**

$$\mathrm{H}_{\mathrm{t}}=\mathrm{O}_{1}, \mathrm{R}_{1}, \mathrm{A}_{1}, \ldots, \mathrm{A}_{\mathrm{t}-1}, \mathrm{O}_{\mathrm{t}}, \mathrm{R}_{\mathrm{t}}$$

Agent 在采取当前动作的时候会依赖于它之前得到的这个历史，**所以你可以把整个游戏的状态看成关于这个历史的函数：**

$\mathrm{S}_{\mathrm{t}}=\mathrm{f}\left(\mathrm{H}_{\mathrm{t}}\right)$

- 观测和状态的关系

  ==观测==是对状态的部分描述，可能会有部分信息的缺失。==状态==是对世界的完整描述。

  举个🌰：一张报纸下压着一百块钱，观测=一张报纸，状态=一张报纸+一百块钱。

在 agent 的内部也有一个函数来更新这个状态。当 agent 的状态跟环境的状态等价的时候，我们就说这个环境是 `full observability`，就是全部可以观测。换句话说，当 agent 能够观察到环境的所有状态时，我们称这个环境是`完全可观测的(fully observed)`。

当 agent 只能看到部分的观测，我们就称这个环境是`部分可观测的(partially observed)`。在这种情况下面，强化学习通常被建模成一个 POMDP 的问题。

大部分情况下我们遇到的都是部分可观测的环境，即POMDP的问题。

POMDP 可以用一个 7 元组描述：$(\mathrm{S}, \mathrm{A}, \mathrm{T}, \mathrm{R}, \Omega, \mathrm{O}, \gamma)$，其中 S 表示状态空间，为隐变量，A 为动作空间，$\mathrm{T}\left(\mathrm{s}^{\prime} \mid \mathrm{s}, \mathrm{a}\right)$ 为状态转移概率，R 为奖励函数，$\Omega(o|s,a)$ 为观测概率，O 为观测空间，$\gamma $为折扣系数。

### Note

- [ ] 主要的各项参数分别代表什么？需要进一步说明

## Action Space

不同的环境允许不同种类的动作。在给定的环境中，有效动作的集合经常被称为`动作空间(action space)`。像 Atari 和 Go 这样的环境有`离散动作空间(discrete action spaces)`，在这个动作空间里，agent 的动作数量是有限的。在其他环境，比如在物理世界中控制一个 agent，在这个环境中就有`连续动作空间(continuous action spaces)` 。在连续空间中，动作是实值的向量。

举个🌰

- 走迷宫机器人如果只有东南西北这 4 种移动方式，则其为离散动作空间；
- 如果机器人向$ 360^{\circ}$ 中的任意角度都可以移动，则为连续动作空间。

## Major Components of RL Agent

- 首先 agent 有一个 `policy function`，agent 会用这个函数来选取下一步的动作。

- 然后它也可能生成一个`价值函数(value function)`。我们用价值函数来对当前状态进行估价，它就是说你进入现在这个状态，可以对你后面的收益带来多大的影响。当这个价值函数大的时候，说明你进入这个状态越有利。

  ==policy function==和==value function==之间可以相互转换。

- 另外一个组成成分是`模型(model)`。模型表示了 agent 对这个环境的状态进行了理解，它决定了这个世界是如何进行的。

### Policy

Policy 决定了这个 agent 的行为，它其实是一个函数，把输入的状态变成行为。这里有两种 policy：

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.26.png)

- 一种是 `stochastic policy(随机性策略)`，它就是 $\pi$ 函数 $\pi(a | s)=P\left[A_{t}=a | S_{t}=s\right]$ 。当你输入一个状态 s的时候，输出是一个概率。这个概率就是你所有行为的一个概率，然后你可以进一步对这个概率分布进行采样，得到真实的你采取的行为。比如说这个概率可能是有 70% 的概率往左，30% 的概率往右，那么你通过采样就可以得到一个 action。

  $\pi(a \mid s)=P\left(A_{t}=a \mid S_{t}=s\right)$


- 一种是 `deterministic policy(确定性策略)`，就是说你这里有可能只是采取它的极大化，采取最有可能的动作。你现在这个概率就是事先决定好的。

  $a=\pi(s)$

  从 Atari 游戏来看的话，policy function 的输入就是游戏的一帧，它的输出决定你是往左走或者是往右走。

  通常情况下，强化学习一般使用`随机性策略`。随机性策略有很多优点：

- 在学习时可以通过引入一定随机性来更好地探索环境；

- 随机性策略的动作具有多样性，这一点在多个智能体博弈时也非常重要。采用确定性策略的智能体总是对同样的环境做出相同的动作，会导致它的策略很容易被对手预测。

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.31.png)

### Value Function

**价值函数是一个折扣的未来奖励的加和**，就是你通过进行某一种行为，然后你未来得到多大的奖励。

价值函数是对于未来累积奖励的预测，用于评估给定策略下，状态的好坏.价值函数是一个`标量`，长期而言什么是好的

$v_{\pi}(s)=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots \mid S_{t}=s\right]$

价值函数里面有一个 discount factor。我们希望尽可能在短的时间里面得到尽可能多的奖励。如果我们说十天过后，我给你 100 块钱，跟我现在给你 100 块钱，你肯定更希望我现在就给你 100 块钱，因为你可以把这 100 块钱存在银行里面，你就会有一些利息。所以我们就通过把这个 `discount factor` 放到价值函数的定义里面，后面得到的奖励价值函数的定义其实是一个期望。

对于这个奖励函数，我们另外还有一个 Q 函数。Q 函数里面包含两个变量：状态和动作。所以你未来可以获得多少的奖励，它的这个期望取决于你当前的状态和当前的行为。这个 Q 函数是强化学习算法在学习的一个函数。因为当我们得到这个 Q 函数的过后，进入某一种状态，它最优的行为其实就可以通过这个 Q 函数来得到。

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.32.png)

#### Note

==discount factor==此处用来平衡长远奖励和当前奖励。

### Model

用于模拟环境的行为，预测下一个状态

**模型决定了下一个状态会是什么样的，就是说下一步的状态取决于你当前的状态以及你当前采取的行为。**它由两个部分组成，一个是 probability，它这个转移状态之间是怎么转移的。另外是这个奖励函数，当你在当前状态采取了某一个行为，可以得到多大的奖励。

$\mathcal{P}_{s s^{\prime}}^{a}=\mathbb{P}\left[S_{t+1}=s^{\prime} \mid S_{t}=s, A_{t}=a\right]$

$\mathcal{R}_{s}^{a}=\mathbb{E}\left[R_{t+1} \mid S_{t}=s, A_{t}=a\right]$

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.29.png)

#### Note

- [ ] 马尔可夫过程详细解读 

## 强化学习分类

### 1. 根据是否建立环境模型分类



**1.1 基于模型的强化学习（model-based）**

模型可以被环境所知道，agent可以直接利用模型执行下一步的动作，而无需与实际环境进行交互学习。

比如：围棋、迷宫

具体来说，当智能体知道状态转移函数 $P(s_{t+1}|s_t,a_t)$和奖励函数 $R(s_t,a_t)$后，它就能知道在某一状态下执行某一动作后能带来的奖励和环境的下一状态，这样智能体就不需要在真实环境中采取动作，直接在虚拟世界中学习和规划策略即可。这种学习方法称为`有模型学习`。

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.35.png)

**1.2 模型无关的强化学习（model-free）**

它没有去直接估计这个状态的转移，也没有得到环境的具体转移变量。它通过学习 value function 和 policy function 进行决策。这种 model-free 的模型里面没有一个环境转移的一个模型。

真正意义上的强化学习，环境是黑箱。现实中的情况大多是这种

没有环境模型

比如Atari游戏，需要大量的采样

![img](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/8aee6642-1717-43f8-ac1e-acdcf236a5b3/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20201019%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20201019T070532Z&X-Amz-Expires=86400&X-Amz-Signature=f1c2cb152a4da5b0d4ba89ef2423b96c98d8b8f5f913fca4f03c53d6e677b2f0&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22)

#### Note

- [ ] 有模型强化学习和免模型强化学习有什么区别？

  **核心点在于是否需要对真实环境建模.**

目前，大部分深度强化学习方法都采用了免模型学习，这是因为：

- 免模型学习更为简单直观且有丰富的开源资料，像 DQN、AlphaGo 系列等都采用免模型学习；
- 在目前的强化学习研究中，大部分情况下环境都是静态的、可描述的，智能体的状态是离散的、可观察的（如 Atari 游戏平台），这种相对简单确定的问题并不需要评估状态转移函数和奖励函数，直接采用免模型学习，使用大量的样本进行训练就能获得较好的效果。

### 2. 根据如何获得策略分类

1. **基于价值的强化学习**

   ==没有策略（隐含）、价值函数==

   显式地学习的是价值函数，隐式地学习了它的策略。因为这个策略是从我们学到的价值函数里面推算出来的。

   举个🌰：

   ​	走迷宫游戏，向左向右的**value**分别为👈：20，👉：10

​		    通过比较两者之间的价值，得到👈的动作更有利，于是👈。

2. **基于策略的强化学习**

   ==策略、没有价值函数==

   它直接去学习 policy，就是说你直接给它一个 state，它就会输出这个动作的概率。在这个 policy-based agent 里面并没有去学习它的价值函数。

   举个🌰：

   ​	走迷宫游戏，根据当前的位置，得到向左向右的**概率**分别为👈：67%，👉：33%。

   ​	 根据向左向右的概率，选择动作。

3. **Actor-Critic**

   ==策略、价值函数==

   把 value-based 和 policy-based 结合起来就有了 `Actor-Critic agent`。这一类 agent 就把它的策略函数和价值函数都学习了，然后通过两者的交互得到一个最佳的行为。

#### Note

基于策略迭代和基于价值迭代的强化学习方法有什么区别?

在`基于策略迭代`的强化学习方法中，智能体会`制定一套动作策略`（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。

而在`基于价值迭代`的强化学习方法中，智能体不需要制定显式的策略，它`维护一个价值表格或价值函数`，并通过这个价值表格或价值函数来选取价值最大的动作。**基于价值迭代的方法只能应用在不连续的、离散的环境下（如围棋或某些游戏领域），**对于行为集合规模庞大、动作连续的场景（如机器人控制领域），其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作)。

基于价值迭代的强化学习算法有 Q-learning、 Sarsa 等，而基于策略迭代的强化学习算法有策略梯度算法等。此外， Actor-Critic 算法同时使用策略和价值评估来做出决策，其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，取得更好的效果。

强化学习分类图

![img](https://datawhalechina.github.io/leedeeprl-notes/chapter1/img/1.36.png)



## Exploration and Exploitation

在强化学习里面，`Exploration` 和` Exploitation` 是两个很核心的问题。

- Exploration 是说我们怎么去探索这个环境，通过尝试不同的行为来得到一个最佳的策略，得到最大奖励的策略。
- Exploitation 是说我们不去尝试新的东西，就采取已知的可以得到很大奖励的行为。

因为在刚开始的时候强化学习 agent 不知道它采取了某个行为会发生什么，所以它只能通过试错去探索。所以 Exploration 就是在试错来理解采取的这个行为到底可不可以得到好的奖励。Exploitation 是说我们直接采取已知的可以得到很好奖励的行为。所以这里就面临一个==trade-off==，怎么通过牺牲一些短期的 reward 来获得行为的理解。

举个🌰

​	在一条街吃饭，有20家餐厅，需要怎么去尽可能地找到我们喜欢的那家餐厅。在不同的情况下==trade–off==不一样。

	- 前9家都很难吃，第10家非常好吃。
	- 前3家都很好吃，后面5家很难吃。

## 实践

比较推荐OpenAI和Gym。

在安装的过程中，推荐使用Anaconda+VSCode的方案。相对来说比较轻便。

[Anaconda使用指南](https://www.jianshu.com/p/eaee1fadc1e9)

[Anaconda介绍](https://zhuanlan.zhihu.com/p/32925500)